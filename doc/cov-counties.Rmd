---
title: "Detecting covid-19 cases using machine learning"
subtitle: "County Cross-section"
author: "Hal Varian, Robert On"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_notebook:
    toc: yes
    toc_float: yes
    code_folding: hide
  html_document:
    toc: yes
    df_print: paged
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

Loading required packages.

```{r include=FALSE}
# install.packages("fable", "plotly", "BoomSpikeSlab", "drake", "huxtable",
#                  "tidyverse", "ranger", "missRanger", "pdp", "caret")
spike_slab_iter <- 1000
# interactive html plots
library(plotly)
# spike and slab variable selection
library(BoomSpikeSlab)
# nicely formatted regression tables
library(huxtable)
# fast random forests
library(ranger)
# general data manipulation
library(tidyverse)
```

```{r include=FALSE}
# read in prepared data
nber_dat <- read_csv("https://ond3.com/covdata/nber_dat.csv")
# nber_dat <- read_csv("./Data/nber_dat.csv")
```

## Motivation

We explore the potential of survey-based data as a way to accurately measure the incidence of disease for these purposes. This notebook focuses on a cross-sectional geographical analysis of incidence while a follow up will focus on the time dimension.


## Base model

Hypothesis: Survey data should look like population data.

Our basis model is very simple.  We have two variables,  $pJ$, defined as confirmed cases as a fraction of population from John Hopkins, and $pG$, defined as the fraction of Google Survey respondents who answer “yes” to the proxy question.

Our hypothesis is that $pJ = k*pG + e$ where $k$ is a parameter to be estimated and $e$ is an error term.  We could estimate this equation directly, but we could also use the definitions of $pG$ and $pJ$ to write:
		            
$$
\begin{aligned}
\frac{cases}{population} \sim  \frac{yes}{yes+no}
\end{aligned}
$$
		            
Taking logs:
$$
\begin{aligned}
log(cases) &= log(k) + \beta_1 log(population) + \beta_2 log (pG) + e \\ 
           &= log(k) + \beta_1 log(population) + \beta_2 log (yes) - \beta_3 log(yes+no) + e
\end{aligned}
$$

We can estimate either of these equations depending on data availability.  We can also add various controls such as country health characteristics, lagged values of cases, county fixed effects, and so on.  But even if we use this very simple specification, we get nice results, as shown in this plot of actual versus predicted cases.

```{r}
goog_mdl <- lm(log(jhu_csse_confirmed_incidence_num_value + 1) ~
     log(google_survey_raw_cli_value + 1) +
       log(google_survey_raw_cli_sample_size + 1) +
       log(population + 1), 
   data = nber_dat %>% column_to_rownames("county_name"))

huxreg(goog_mdl, statistics = c(N = "nobs", R2 = "r.squared"))

data.frame(fitted = goog_mdl$fitted.values, actual = goog_mdl$model[,1],
           population = goog_mdl$model[,4], fips = rownames(goog_mdl$model)) %>%
  ggplot(aes(actual, fitted, label = fips)) + geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) + 
  theme(legend.position = "none") + coord_fixed() -> p
ggplotly(p)
```

### Train/test

What is common in the machine learning is to keep our predictive models "honest" by estimating its performance on data the model was not trained with. Here we randomly split our data into a training set with 75% of the observations and 25% for the test set.

```{r}
nber_train_test <- nber_dat %>% mutate(
  sample_type = if_else(rbinom(n(), 1, 0.75) == 1, "train", "test"))

goog_mdl_train <- lm(log(jhu_csse_confirmed_incidence_num_value + 1) ~
     log(google_survey_raw_cli_value + 1) +
       log(google_survey_raw_cli_sample_size + 1) +
       log(population + 1), 
   data = nber_train_test %>%
     column_to_rownames("county_name") %>%
     filter(sample_type == "train"))

goog_mdl_test <- data.frame(
  predicted = predict(
    goog_mdl_train,
    nber_train_test %>% filter(sample_type == "test")),
  actual = nber_train_test %>% filter(sample_type == "test") %>%
    select(jhu_csse_confirmed_incidence_num_value) %>%
    mutate_all(~ log(.x + 1)) %>%
    pull(jhu_csse_confirmed_incidence_num_value)) %>%
  filter(complete.cases(.))

goog_test_r2 <- goog_mdl_test %>%
  mutate(resid_sq = (actual - predicted)^2) %>%
  summarise(rss = sum(resid_sq), var_y = var(actual) * n()) %>%
  mutate(r2 = 1 - rss / var_y) %>% pull(r2) %>%
  round(2)

goog_mdl_test %>% ggplot(aes(predicted, actual)) + geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) + 
  ggtitle(paste("test r^2 =", goog_test_r2)) -> p
ggplotly(p)
```

Our out-of-sample $r^2$ is in the same range as our training model giving us confidence we did not overfit, however given the simplicity of the model it was unlikely we would.

### Proxy questions

Earlier we saw how well the community-based question in Google surveys proxied for county-level incidence but how would this compare to a question that simply asked the question directly to an individual? An analogous set of surveys run by Facebook asked the question at the individual level which initially did not have much predictive value until it was changed to mimic the community-based Google question.

```{r}
fb_indiv_spec <- lm(
  log(jhu_csse_confirmed_incidence_num_value + 1) ~
    log(fb_survey_raw_cli_value + 1) +
    log(fb_survey_raw_cli_sample_size + 1) +
    log(population + 1),
  data = nber_dat %>% filter(!is.na(fb_survey_raw_hh_cmnty_cli_value)))

fb_comm_spec <- lm(
  log(jhu_csse_confirmed_incidence_num_value + 1) ~
    log(fb_survey_raw_hh_cmnty_cli_value + 1) +
    log(fb_survey_raw_hh_cmnty_cli_sample_size + 1) +
    log(population + 1),
   data = nber_dat %>% column_to_rownames("county_name"))

fb_comm_nohh_spec <- lm(log(jhu_csse_confirmed_incidence_num_value + 1) ~
     log(fb_survey_raw_nohh_cmnty_cli_value + 1) +
       log(fb_survey_raw_nohh_cmnty_cli_sample_size + 1) +
       log(population + 1),
   data = nber_dat %>% column_to_rownames("county_name"))

ht <- huxreg(fb_indiv_spec, fb_comm_spec, fb_comm_nohh_spec,
             statistics = c(N = "nobs", R2 = "r.squared"))
position(ht) <- "left"
ht
```

### Additional signals

The COVIDcast API includes a number of other signals that could contain information on covid incidence.

```{r}
all_mdl <- lm(log(jhu_csse_confirmed_incidence_num_value + 1) ~
     log(population + 1) +
     log(google_survey_raw_cli_value + 1) +
       log(google_survey_raw_cli_sample_size + 1) +
     log(fb_survey_smoothed_nohh_cmnty_cli_value + 1) +
       log(fb_survey_smoothed_nohh_cmnty_cli_sample_size + 1) +
     log(safegraph_completely_home_prop_value + 1) +
     log(safegraph_full_time_work_prop_value + 1) +
     log(safegraph_median_home_dwell_time_value + 1) +
     log(safegraph_part_time_work_prop_value + 1) +
     log(safegraph_median_home_dwell_time_value + 1),  
   data = nber_dat %>%
     mutate(population = if_else(county_name == "District of Columbia, NA",
                                 705749, population)) %>%
     column_to_rownames("county_name") %>%
     filter(!is.na(google_survey_raw_cli_value)) %>%
     select_if(~ is.numeric(.x) & mean(!is.na(.x)) == 1)) %>% summary()

huxreg(all_mdl, statistics = c(N = "nobs", R2 = "r.squared"))
```

When we include all the other signals, $r^2$ improves 4.5pp.

## County health rankings

[CountyHealthRankings.org](https://www.countyhealthrankings.org/) contains demographic information for every county in the US.

We'd like to understand how county characterics explain some of the remaining variation in covid cases. We use the fitted values from the regression of the average daily incidence of covid on the the number of reported cases as specified in the base model. We regress the average daily incidence of covid on these fitted values in addition to the county covariates.

We include the predicted values from the Google survey data to examine what additional county covariates are significant in explaining the remaining variation.

### Predicting confirmed cases

#### Full OLS

```{r}
covar_dat <- nber_dat %>%
  inner_join(data.frame(county_name = names(goog_mdl$fitted),
                        predicted = goog_mdl$fitted),
             by = "county_name") %>%
  select(jhu_csse_confirmed_incidence_num_value, predicted,
         pct_65_and_older:year_structure_built, -pct_non_hispanic_white,
         -starts_with("uninsured_")) %>%
  select_if(~ mean(!is.na(.x)) == 1) %>% mutate_all(~ log(.x + 1))

covar_mdl <- lm(jhu_csse_confirmed_incidence_num_value ~ ., covar_dat)

huxreg(covar_mdl, statistics = c(N = "nobs", R2 = "r.squared"))
```

A few predictors are significant for this specification. Minority populations, Native Americans and non-Hispanic Black appears to predict higher cases, as does increased black/white residential racial segregation.  Health indicators such as physical inactivity also appear to predict higher cases. Food insecurity and driving alone to work appear to predict less cases. Driving alone to work could be suggestive of the role crowded, public transit plays in disease spread.  Food insecurity could be a measure of poverty and depressed economic activity which could also limit disease spread. Greater flu vaccinations also predict higher cases which may suggest a population that is more open or accessible to disease testing facilities.

#### Spike-slab

Now do variable selection with spike-slab.

```{r}
ss_mdl <- lm.spike(jhu_csse_confirmed_incidence_num_value ~ .,
                   niter = spike_slab_iter,
                   data = covar_dat)
```

```{r}
ss_rsq <- summary(ss_mdl)$rsquare %>% .[["Mean"]] %>% setNames("rsquare")
ss_mdl %>% plot("inclusion",
                 inclusion.threshold = 0.5,
                 main = paste("r-square:", round(ss_rsq, 2)))
```

Many of the same predictors are picked up by spike and slab.

### Predicting deaths

We repeat the same analysis above but now trying to predict deaths.

#### Full OLS

We do regular OLS with all county covariates and the confirmed case predictions from the Google survey data.

```{r}
covar_dat <- nber_dat %>%
  inner_join(data.frame(county_name = names(goog_mdl$fitted),
                        predicted = goog_mdl$fitted),
             by = "county_name") %>%
  select(jhu_csse_deaths_incidence_num_value, predicted,
         pct_65_and_older:year_structure_built, -pct_non_hispanic_white,
         -starts_with("uninsured_")) %>%
  select_if(~ mean(!is.na(.x)) == 1) %>% mutate_all(~ log(.x + 1))

covar_mdl <- lm(jhu_csse_deaths_incidence_num_value ~ ., covar_dat)

huxreg(covar_mdl, statistics = c(N = "nobs", R2 = "r.squared"))
```

A few predictors are significant for this specification. Native American populations appears to predict higher cases while higher proportions of Asians predict lower cases.  Health indicators such as physical inactivity also appear to predict higher cases. Food insecurity and driving alone to work appear to predict less cases. Driving alone to work could be suggestive of the role crowded, public transit plays in disease spread.  Food insecurity could be a measure of poverty and depressed economic activity which could also limit disease spread.

#### Spike-slab

Now do variable selection with spike-slab.

```{r}
ss_mdl <- lm.spike(jhu_csse_deaths_incidence_num_value ~ .,
                   niter = spike_slab_iter,
                   data = covar_dat)
```

```{r}
ss_rsq <- summary(ss_mdl)$rsquare %>% .[["Mean"]] %>% setNames("rsquare")
ss_mdl %>% plot("inclusion",
                 inclusion.threshold = 0.5,
                 main = paste("r-square:", round(ss_rsq, 2)))
```

Many of the same predictors are shown but with more indicators related to health. Income inequality and racial segregation predict higher cases. Driving alone to work and food insecurity continue to predict fewer cases. Asian populations continue to predict lower case numbers.

## Comparison to Knittel and Ozaltun and Wu, et al

[Knittel and Ozaltun, 2020](https://www.nber.org/papers/w27391.pdf) use a similar set of correlates from the American Communit Survey (ACS) with deaths that does not use a a measure of infection. They find higher shares of African American residents correlate with higher deaths per capita across states, but not within states. A similar effect is seen with the elderly. Higher amounts of public transport and driving into work relative to telecommuting predict higher deaths. Higher share of people nto working, home values, higher summer temperatures, and lower winter temperatures also have higher death rates.

Unlike [Wu, et al 2020](https://www.medrxiv.org/content/10.1101/2020.04.05.20054502v2) they do not find air pollution as a factor. Futhermore they do not find that obsesity, number of ICU beds per capita, or poverty rates.

![regression table](knittel_ozaltun.png)

Our results are not directly comparable because we control for the number of surveyed infections from the Google survey. This allows us to examine the predictors of deaths controlling for infection rates.  This surfaces a slightly different set of predictors that include positive drivers of deaths: racial segregation, population, physical inactivity, and income inequality.  The negative drivers of deaths include: food insecurity, driving alone to work, a greater share of Asians, and poor or fair health.

This suggests that areas with a sufficient amount of income equality and racial segregation seem to drive more deaths whille areas that are more inpoverished overall drive less deaths.  Driving alone to work suggests that commuting plays a major role in COVID mortality.


## Appendix

### Using 595 counties is fine.

The Facebook and Google surveys cover less than 20% of
counties. However if we look at the covered population of these
counties we find that, by population, these counties cover more
than 75% of the population in the US.

```{r}
nber_dat %>%
  select(population, contains("survey") & ends_with("_value")) %>%
  gather(metric, value, -population) %>%
  group_by(metric) %>%
  summarise(`covered counties` = mean(!is.na(value))) %>%
  mutate_at(vars(metric), ~ str_remove(.x, "_value")) %>%
  ggplot(aes(metric, `covered counties`)) + geom_col() + coord_flip() +
  ylim(0, 1) +
  xlab(element_blank())
```

The Facebook and Google surveys cover less than 20% of counties. However if we look at the covered population of these counties we find that, by population, these counties cover more than 75% of the population in the US.

```{r}
nber_dat %>%
  select(population, contains("survey") & ends_with("_value")) %>%
  gather(metric, value, -population) %>%
  filter(!is.nan(population)) %>%
  group_by(metric) %>%
  summarise(
    `covered population` = sum(population[!is.nan(value)]) / sum(population)) %>%
  mutate_at(vars(metric), ~ str_remove(.x, "_value")) %>%
  ggplot(aes(metric, `covered population`)) + geom_col() + coord_flip() +
  ylim(0, 1) + 
  xlab(element_blank())
```

### Random forest

#### Cases

Not sure if RF is useful here:

```{r}
ranger_mdl <- ranger(jhu_csse_confirmed_incidence_num_value ~ .,
                     data = covar_dat, importance = "impurity")

ranger_mdl %>% importance() %>%
  tibble(var = names(.), importance = .) %>%
  top_n(9, importance) %>%
  mutate(var = fct_reorder(var, importance)) %>%
  ggplot(aes(var, importance)) + geom_bar(stat = "identity") +
  coord_flip() + xlab(element_blank()) +
  ggtitle(paste("r-square:", round(ranger_mdl$r.squared, 2))) +
  theme_minimal()
```

Partial dependance plots for these variables are shown below:

```{r fig.height=8}
top_vars <- importance(ranger_mdl) %>% sort %>% tail(9) %>% rev() %>% names()
pdps <- lapply(top_vars, FUN = function(x) {
  ranger_mdl %>%
    pdp::partial(pred.var = x, train = covar_dat) %>%
    pdp::plotPartial()
})
do.call(gridExtra::grid.arrange, c(pdps, ncol=3))
```

#### Deaths

Not sure if RF is useful here:

```{r}
ranger_mdl <- ranger(jhu_csse_deaths_incidence_num_value ~ .,
                     data = covar_dat, importance = "impurity")

ranger_mdl %>% importance() %>%
  tibble(var = names(.), importance = .) %>%
  top_n(9, importance) %>%
  mutate(var = fct_reorder(var, importance)) %>%
  ggplot(aes(var, importance)) + geom_bar(stat = "identity") +
  coord_flip() + xlab(element_blank()) +
  ggtitle(paste("r-square:", round(ranger_mdl$r.squared, 2))) +
  theme_minimal()
```

Partial dependance plots for these variables are shown below:

```{r fig.height=8}
top_vars <- importance(ranger_mdl) %>% sort %>% tail(9) %>% rev() %>% names()
pdps <- lapply(top_vars, FUN = function(x) {
  ranger_mdl %>%
    pdp::partial(pred.var = x, train = covar_dat) %>%
    pdp::plotPartial()
})
do.call(gridExtra::grid.arrange, c(pdps, ncol=3))
```
