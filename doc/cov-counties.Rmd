---
title: "Nowcasting covid-19 cases"
author: "Hal Varian, Robert On"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
    toc_float: yes
    code_folding: hide
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

Loading required packages.

```{r include=FALSE}
# install.packages("fable", "plotly", "BoomSpikeSlab", "drake", "huxtable",
#                  "tidyverse", "ranger", "missRanger", "pdp", "caret")
spike_slab_iter <- 2000
# interactive html plots
library(plotly)
# spike and slab variable selection
library(BoomSpikeSlab)
# nicely formatted regression tables
library(huxtable)
# fast random forests
library(ranger)
# general data manipulation
library(tidyverse)
```
```{r include=FALSE,label="read data"}
# read in prepared data
nber_dat <- read_csv("https://ond3.com/covdata/nber_dat.csv")
# nber_dat <- read_csv("./Data/nber_dat.csv")
```

## Abstract

We explore the potential of survey data for early detection of
covid-19 at the county level.  Our most important result is that
"proxy questions" about symptoms at a community level are significantly
better predictors than questions about symptoms at the household
level.  We also examine a number of other cross-sectional and time
series predictors at the county level.

## Methodology

There are two approaches to prediction in economics: structural
modeling and reduced form modeling.  Structural models are concerned
with causal relationships, the impact of interventions, and long-term
forecasts.  Reduced forms generally consider short-run forecasts, and
rely on correlation and extrapolation.  The goal here is to get the
most accurate short-run predictions

An example of a structural models is the famous SIR model [list of
cites].  This is emphatically not the goal of this note.  Our focus is
only on short run prediction [list of cites].

## Survey design

In February 2020, the [Delphi project](https://delphi.cmu.edu/) was
asked by the CDC to build a county-level predictive model for covid-19
confirmed cases. Ryan Tibshirani, co-leader of Delphi felt strongly
that survey level symptom data would be helpful in this project.

In mid-March 2020 we worked with the Delphi group to launch a
one-question survey asking "Do you or anyone in your household have a
fever of 100 degrees F or higher along with a sore throat or cough?”
This question was modeled on the CDC definition of
influenza-like-illness used in the [Influenza Surveillance
System](https://www.cdc.gov/flu/weekly/overview.htm). From now on we
will refer to this as the "household question".

The survey responses turned out to be very helpful as predictors of
confirmed cases, as we will show below.  According to Professor
Tibshirani it was the best predictor they had.  A simple univariate
linear regression between the question response and cases yielded an
$R^2$ of 0.59.

Unfortunately this survey had to be discontinued at the end of March
due to concerns about the question wording.  Brett Slakin, the
co-creator of Google Surveys suggested using a proxy question of the
form "Do you know of someone in your community who is sick with a
fever, along with cough, shortness of breath, or difficulty breathing
right now?”  We will refer to this question as the "community
question".

Our expectation was that this somewhat vague question would not perform
as well as the household questions, but we turned out to be dead
wrong: the community question had an $R^2$ of 0.81, which was quite an
improvement over the previous question.  There are also a number of
other advantages of this proxy question has over the traditional
household question.  We will discuss this below, after we take a look
at the simple analytic models we used.

## Basic model

Our basis model is very simple.  We have two variables, $pJ$, defined
as confirmed cases as a fraction of population from John Hopkins, and
$pG$, defined as the fraction of Google Survey respondents who answer
“yes” to the survey question.  Note that the survey question had three
responses, "yes", "no", and "prefer not to answer".  We found that
using the definitive responses, "yes" and "no", gave us a better fit
than including the "prefer not to answer" response.

Our initial regression specification is $$pJ_{ct} = k*pG_{ct} +
  e_{ct}$$ where $k$ is a parameter to be estimated and $e_{ct}$ is an
  error term.  We could estimate this equation directly, but it is
  relatively inflexible. Instead, we use the definitions of $pG$ and
  $pJ$ to write:
		            
$$
\begin{aligned}
\frac{cases}{population} \sim  \frac{yes}{yes+no}
\end{aligned}
$$
		            
Taking logs:
$$
\begin{aligned}
log(cases) &= log(k) + \beta_1 log(population) + \beta_2 log (pG) + e \\ 
           &= log(k) + \beta_1 log(population) + \beta_2 log (yes) - \beta_3 log(yes+no) + e
\end{aligned}
$$

We can estimate any of these equations depending on data availability.
We can also add various controls such as country health
characteristics, lagged values of cases, county fixed effects, and so
on.  But even if we use this very simple specification, we get nice
results, as shown in this plot of actual versus predicted cases.

```{r, label="basic model"}
goog_mdl <- lm(log(jhu_csse_confirmed_incidence_num_value + 1) ~
     log(google_survey_raw_cli_value + 1) +
       log(google_survey_raw_cli_sample_size + 1) +
       log(population + 1), 
   data = nber_dat %>% column_to_rownames("county_name"))

huxreg(goog_mdl, statistics = c(N = "nobs", R2 = "r.squared"))

data.frame(fitted = goog_mdl$fitted.values, actual = goog_mdl$model[,1],
           population = goog_mdl$model[,4], fips = rownames(goog_mdl$model)) %>%
  ggplot(aes(actual, fitted, label = fips)) + geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) + 
  theme(legend.position = "none") + coord_fixed() -> p
ggplotly(p)
```

### Train/test

The reported results are in-sample fit, which is fine for this simple
regression.  But just to be safe, we randomly split our data into a
training set with 75% of the observations and 25% for a test set,
which gave us essentially the same results as in the in-sample fit.
We also used cross validation which is not reported but also gave us
similar results.

```{r,label="train/test"}
nber_train_test <- nber_dat %>% mutate(
  sample_type = if_else(rbinom(n(), 1, 0.75) == 1, "train", "test"))

goog_mdl_train <- lm(log(jhu_csse_confirmed_incidence_num_value + 1) ~
     log(google_survey_raw_cli_value + 1) +
       log(google_survey_raw_cli_sample_size + 1) +
       log(population + 1), 
   data = nber_train_test %>%
     column_to_rownames("county_name") %>%
     filter(sample_type == "train"))

goog_mdl_test <- data.frame(
  predicted = predict(
    goog_mdl_train,
    nber_train_test %>% filter(sample_type == "test")),
  actual = nber_train_test %>% filter(sample_type == "test") %>%
    select(jhu_csse_confirmed_incidence_num_value) %>%
    mutate_all(~ log(.x + 1)) %>%
    pull(jhu_csse_confirmed_incidence_num_value)) %>%
  filter(complete.cases(.))

goog_test_r2 <- goog_mdl_test %>%
  mutate(resid_sq = (actual - predicted)^2) %>%
  summarise(rss = sum(resid_sq), var_y = var(actual) * n()) %>%
  mutate(r2 = 1 - rss / var_y) %>% pull(r2) %>%
  round(2)

goog_mdl_test %>% ggplot(aes(predicted, actual)) + geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) + 
  ggtitle(paste("test r^2 =", goog_test_r2)) -> p
ggplotly(p)
```

### Proxy experiment

In early April, Facebook started encouraging their users to go to a
new survey created by the Delphi group at CMU.  This gave us a chance
to have a direct comparison between the household and the community
questions.  Below we compare the results of three treatments household
alone, community alone, and both together.  Note that the community
question alone was also somewhat better than having both questions.

This suggests that in future applications, one could use both
questions initially and then choose the question that performed best.

```{r,label="proxy experiment"}
fb_indiv_spec <- lm(
  log(jhu_csse_confirmed_incidence_num_value + 1) ~
    log(fb_survey_raw_cli_value + 1) +
    log(fb_survey_raw_cli_sample_size + 1) +
    log(population + 1),
  data = nber_dat %>% filter(!is.na(fb_survey_raw_hh_cmnty_cli_value)))

fb_comm_spec <- lm(
  log(jhu_csse_confirmed_incidence_num_value + 1) ~
    log(fb_survey_raw_hh_cmnty_cli_value + 1) +
    log(fb_survey_raw_hh_cmnty_cli_sample_size + 1) +
    log(population + 1),
   data = nber_dat %>% column_to_rownames("county_name"))

fb_comm_nohh_spec <- lm(log(jhu_csse_confirmed_incidence_num_value + 1) ~
     log(fb_survey_raw_nohh_cmnty_cli_value + 1) +
       log(fb_survey_raw_nohh_cmnty_cli_sample_size + 1) +
       log(population + 1),
   data = nber_dat %>% column_to_rownames("county_name"))

ht <- huxreg(fb_indiv_spec, fb_comm_spec, fb_comm_nohh_spec,
             statistics = c(N = "nobs", R2 = "r.squared"))
position(ht) <- "left"
ht
```

### Additional predictors

Up until now we have looked at regressions using survey responses.
However the Delphi website, [covidcast.cmu.edu](covidcast.cmu.edu) has
other predictors, including doctor visits, hospital admissions, away
from home, Google Trends searches, and so on.  Below we run a
regression with these predictors and observe only a small increase in
$R^2$ from these additional prdictors.


```{r}
all_mdl <- lm(log(jhu_csse_confirmed_incidence_num_value + 1) ~
     log(population + 1) +
     log(google_survey_raw_cli_value + 1) +
       log(google_survey_raw_cli_sample_size + 1) +
     log(fb_survey_smoothed_nohh_cmnty_cli_value + 1) +
       log(fb_survey_smoothed_nohh_cmnty_cli_sample_size + 1) +
     log(safegraph_completely_home_prop_value + 1) +
     log(safegraph_full_time_work_prop_value + 1) +
     log(safegraph_median_home_dwell_time_value + 1) +
     log(safegraph_part_time_work_prop_value + 1) +
     log(safegraph_median_home_dwell_time_value + 1),  
   data = nber_dat %>%
     mutate(population = if_else(county_name == "District of Columbia, NA",
                                 705749, population)) %>%
     column_to_rownames("county_name") %>%
     filter(!is.na(google_survey_raw_cli_value)) %>%
     select_if(~ is.numeric(.x) & mean(!is.na(.x)) == 1)) %>% summary()

huxreg(all_mdl, statistics = c(N = "nobs", R2 = "r.squared"))
```
### Proxy question discussion

The proxy approach seems to offer several advantages over the
household approach.  First, as we have shown above it provides more
accurate predictions, a feature of proxy questions that has been observed
in other contexts.  For example, suppose you are interested in
automobile demand in a particular region.  Since buying a car is a
rare event, you would have to ask many individuals in order to get a
good estimate of purchase intent.  But if you ask "Do you know
anyone that is shopping for a car?" you may get a much more useful
survey result.

Second, a proxy question may be useful when asking questions about
sensitive issues such as health.  Third, response rates may increase
since respondents might be willing to talk more about their community
than about themselves.  Fourth, proxy questions may be useful when it
is costly or difficult to locate the respondent you seek.  [Cite the
medical literature, cite the Census bureau paper.]

The bottom line is that proxy questions can be very useful in survey
research and should be more widely used.

## County health rankings and variable selection

[CountyHealthRankings.org](https://www.countyhealthrankings.org/)
contains county-level data for 202 health indicators.  Some of these
indicators are available for all 3218 counties.  However, we have
found that for most purposes 595 counties is adequate, as this
represents about 75% of the population.

We view this as a model selection issue, and utilize a spike-and-slab
regression.  This is a Bayesian approach the estimates the probability
of inclusion in a regression, which makes it particularly easy to
interpret.

```{r, label="death" }
covar_dat <- nber_dat %>%
  inner_join(data.frame(county_name = names(goog_mdl$fitted),
                        predicted = goog_mdl$fitted),
             by = "county_name") %>%
  select(jhu_csse_confirmed_incidence_num_value, predicted,
         pct_65_and_older:year_structure_built, -pct_non_hispanic_white,
         -starts_with("uninsured_")) %>%
  select_if(~ mean(!is.na(.x)) == 1) %>% mutate_all(~ log(.x + 1))
ss_mdl <- lm.spike(jhu_csse_confirmed_incidence_num_value ~ .,
                   niter = spike_slab_iter,
                   data = covar_dat)
```

```{r}
ss_rsq <- summary(ss_mdl)$rsquare %>% .[["Mean"]] %>% setNames("rsquare")
ss_mdl %>% plot("inclusion",
                 inclusion.threshold = 0.5,
                 main = paste("r-square:", round(ss_rsq, 2)))
```

We also ran a kitchen-sink regression with all 202 predictors and
identified those with large t-statistics (the "three-star variables").
Both approaches identified the same predictors.  Of course, no causal
interpretation should be given to these findings.

### Predicting deaths

Here we repeat the estimation described above but we use deaths as the
depending variable.

```{r}
covar_dat <- nber_dat %>%
  inner_join(data.frame(county_name = names(goog_mdl$fitted),
                        predicted = goog_mdl$fitted),
             by = "county_name") %>%
  select(jhu_csse_deaths_incidence_num_value, predicted,
         pct_65_and_older:year_structure_built, -pct_non_hispanic_white,
         -starts_with("uninsured_")) %>%
  select_if(~ mean(!is.na(.x)) == 1) %>% mutate_all(~ log(.x + 1))
```

```{r}
ss_mdl <- lm.spike(jhu_csse_deaths_incidence_num_value ~ .,
                   niter = spike_slab_iter,
                   data = covar_dat)
```

```{r}
ss_rsq <- summary(ss_mdl)$rsquare %>% .[["Mean"]] %>% setNames("rsquare")
ss_mdl %>% plot("inclusion",
                 inclusion.threshold = 0.5,
                 main = paste("r-square:", round(ss_rsq, 2)))
```

## Comparison to other covariate studies

[Knittel and Ozaltun, 2020](https://www.nber.org/papers/w27391.pdf)
use a similar set of correlates from the American Community Survey
(ACS) to examine predictors of deaths.  They find higher shares of
African American residents correlate with higher deaths per capita
across states, but not within states. A similar effect is seen for the
elderly. Higher amounts of public transport and driving to work
relative to telecommuting predict higher deaths. Higher share of
people not working, home values, higher summer temperatures, and lower
winter temperatures are also correlated with higher death rates.

Unlike [Wu, et al
2020](https://www.medrxiv.org/content/10.1101/2020.04.05.20054502v2)
they do not find air pollution as a factor. Furthermore they do not
find that obesity, number of ICU beds per capita, or poverty rates
are important covariates.

Our results are not directly comparable to these studies because we
control for the number of surveyed infections from the Google
survey. This allows us to examine the predictors of deaths while
controlling for infection rates.  This surfaces a slightly different
set of predictors that include positive drivers of deaths: racial
segregation, population, physical inactivity, and income inequality.
The negative drivers of deaths include: food insecurity, driving alone
to work, a greater share of Asians, and poor or fair health.

This suggests that areas with a sufficient amount of income equality
and racial segregation seem to drive more deaths while areas that are
more impoverished overall drive less deaths.  Driving alone to work
suggests that commuting plays a major role in COVID mortality.

## Time series
